import streamlit as st
import os
import tempfile
from pathlib import Path
import shutil
import subprocess

# Check FFmpeg availability
def check_ffmpeg():
    try:
        subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
        return True
    except:
        return False

# Lightweight imports
try:
    import whisper
    WHISPER_AVAILABLE = True
except:
    WHISPER_AVAILABLE = False
    st.error("Whisper not available in cloud environment")

# Audio loading fallback without FFmpeg
def load_audio_fallback(file_path):
    """Load audio without FFmpeg using librosa or soundfile"""
    try:
        import librosa
        audio, sr = librosa.load(file_path, sr=16000)
        return audio
    except:
        try:
            import soundfile as sf
            audio, sr = sf.read(file_path)
            # Resample if needed
            if sr != 16000:
                import librosa
                audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
            return audio
        except Exception as e:
            st.error(f"Audio loading failed: {e}")
            return None

def format_timestamp(seconds):
    """Convert seconds to MM:SS format"""
    minutes = int(seconds // 60)
    seconds = int(seconds % 60)
    return f"{minutes:02d}:{seconds:02d}"

def detect_speakers_from_text(text):
    """Detect dialogue patterns and estimate speaker count"""
    # Look for dialogue indicators
    dialogue_patterns = [
        "- ", "â€¢ ", "Speaker", "Host:", "Guest:", 
        "A:", "B:", "1:", "2:", "Q:", "A:"
    ]
    
    lines = text.split('\n')
    speaker_changes = 0
    
    for line in lines:
        for pattern in dialogue_patterns:
            if pattern in line:
                speaker_changes += 1
                break
    
    # Estimate speakers based on content length and patterns
    if speaker_changes > 5:
        return min(speaker_changes // 3, 4)  # Max 4 speakers
    elif "dialogue" in text.lower() or "conversation" in text.lower():
        return 2
    else:
        return 1

def display_timestamped_transcript(segments):
    """Display transcript with timestamps and speaker detection"""
    st.subheader("ğŸ“ Timestamped Transcript")
    
    # Analyze for dialogue patterns
    full_text = " ".join([seg["text"] for seg in segments])
    estimated_speakers = detect_speakers_from_text(full_text)
    
    if estimated_speakers > 1:
        st.info(f"ğŸ—£ï¸ Detected dialogue with approximately {estimated_speakers} speakers")
    
    # Create columns for better layout
    col1, col2 = st.columns([1, 4])
    
    current_speaker = 1
    for i, segment in enumerate(segments):
        # Simple speaker alternation for dialogue
        if estimated_speakers > 1 and i > 0:
            # Change speaker every few segments for dialogue
            if i % 3 == 0:  # Rough speaker change estimation
                current_speaker = 2 if current_speaker == 1 else 1
        
        with col1:
            start_time = format_timestamp(segment["start"])
            end_time = format_timestamp(segment["end"])
            if estimated_speakers > 1:
                st.write(f"**ğŸ‘¤ Speaker {current_speaker}**")
                st.write(f"*{start_time}-{end_time}*")
            else:
                st.write(f"**{start_time}-{end_time}**")
        
        with col2:
            text = segment["text"].strip()
            if estimated_speakers > 1:
                st.write(f"ğŸ’¬ {text}")
            else:
                st.write(text)
        
        st.divider()

def main():
    st.title("ğŸ™ï¸ AI Podcast Video Generator 2025")
    st.write("ğŸ¤– No-code AI automation: Audio â†’ Transcript â†’ Video")
    
    # Check system dependencies
    if not check_ffmpeg():
        st.warning("âš ï¸ FFmpeg not detected. Using fallback audio loading.")
    
    # Sidebar for options
    with st.sidebar:
        st.header("ğŸ›ï¸ Settings")
        model_size = st.selectbox("Whisper Model", ["tiny", "base", "small"], index=1)
        show_timestamps = st.checkbox("Show timestamps", value=True)
        generate_video = st.checkbox("Enable video generation", value=False)
        
        if generate_video:
            st.info("ğŸ¬ Video generation will be available after transcript")
    
    # File uploads
    st.header("ğŸ“ Upload Files")
    col1, col2 = st.columns(2)
    
    with col1:
        audio_file = st.file_uploader(
            "ğŸµ Upload audio file", 
            type=["wav", "mp3", "m4a", "ogg"],
            help="Upload podcast or audio file"
        )
    
    with col2:
        if generate_video:
            st.subheader("ğŸ‘¥ Speaker Photos")
            face_image1 = st.file_uploader(
                "ğŸ‘¤ Speaker 1 photo",
                type=["jpg", "png", "jpeg"],
                help="Photo of the first speaker",
                key="speaker1_upload"
            )
            
            face_image2 = st.file_uploader(
                "ğŸ‘¤ Speaker 2 photo", 
                type=["jpg", "png", "jpeg"],
                help="Photo of the second speaker",
                key="speaker2_upload"
            )
            
            # Optional: More speakers
            add_more_speakers = st.checkbox("â• More speakers?")
            if add_more_speakers:
                face_image3 = st.file_uploader(
                    "ğŸ‘¤ Speaker 3 photo",
                    type=["jpg", "png", "jpeg"],
                    key="speaker3_upload"
                )
    
    if audio_file and WHISPER_AVAILABLE:
        st.success(f"âœ… Uploaded: {audio_file.name}")
        
        if st.button("ğŸ¯ Generate Transcript", type="primary"):
            with st.spinner("ğŸ¤– AI processing your audio..."):
                try:
                    # Save uploaded file
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                        tmp_file.write(audio_file.read())
                        temp_path = tmp_file.name
                    
                    # Load model
                    st.info(f"Loading {model_size} model...")
                    model = whisper.load_model(model_size)
                    
                    # Try standard transcription first
                    try:
                        result = model.transcribe(temp_path, verbose=True)
                        
                        if show_timestamps:
                            # Display timestamped transcript
                            display_timestamped_transcript(result["segments"])
                            
                            # Download options
                            st.subheader("ğŸ’¾ Download Options")
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                # Simple transcript
                                simple_transcript = result["text"]
                                st.download_button(
                                    "ğŸ“„ Download Simple Transcript",
                                    simple_transcript,
                                    file_name=f"transcript_{audio_file.name}.txt",
                                    mime="text/plain"
                                )
                            
                            with col2:
                                # Timestamped transcript
                                timestamped_text = ""
                                for segment in result["segments"]:
                                    start = format_timestamp(segment["start"])
                                    end = format_timestamp(segment["end"])
                                    text = segment["text"].strip()
                                    timestamped_text += f"[{start}-{end}] {text}\n\n"
                                
                                st.download_button(
                                    "â° Download Timestamped Transcript",
                                    timestamped_text,
                                    file_name=f"timestamped_{audio_file.name}.txt",
                                    mime="text/plain"
                                )
                        else:
                            # Simple transcript display
                            st.subheader("ğŸ“ Transcript")
                            st.text_area("Generated Transcript", result["text"], height=300)
                        
                        # Video generation section
                        if generate_video:
                            st.header("ğŸ¬ Video Generation")
                            
                            # Check if any speaker photos uploaded
                            speakers_uploaded = []
                            if 'face_image1' in locals() and face_image1:
                                speakers_uploaded.append("Speaker 1")
                            if 'face_image2' in locals() and face_image2:
                                speakers_uploaded.append("Speaker 2")
                            if 'face_image3' in locals() and face_image3:
                                speakers_uploaded.append("Speaker 3")
                            
                            if speakers_uploaded:
                                st.success(f"âœ… Photos uploaded for: {', '.join(speakers_uploaded)}")
                                
                                # Speaker detection from transcript
                                speaker_count = len([seg for seg in result["segments"] if "speaker" in seg.get("text", "").lower()])
                                if speaker_count == 0:
                                    # Estimate speakers from dialogue patterns
                                    speaker_count = 2  # Default for dialogue
                                
                                st.info(f"ğŸ—£ï¸ Detected approximately {speaker_count} speakers in audio")
                                
                                col_vid1, col_vid2 = st.columns(2)
                                
                                with col_vid1:
                                    if st.button("ğŸ¥ Generate Dialogue Video"):
                                        st.balloons()
                                        with st.spinner("ğŸ¬ Creating dialogue video..."):
                                            # Simulate video generation process
                                            progress = st.progress(0)
                                            status = st.empty()
                                            
                                            steps = [
                                                ("ğŸ” Analyzing dialogue structure...", 20),
                                                ("ğŸ‘¥ Matching speakers to photos...", 40),
                                                ("ğŸ­ Generating avatar animations...", 60), 
                                                ("ğŸ¬ Rendering video scenes...", 80),
                                                ("âœ¨ Adding transitions...", 100)
                                            ]
                                            
                                            for step_text, prog in steps:
                                                status.text(step_text)
                                                progress.progress(prog)
                                                
                                            st.success("ğŸ‰ Dialogue video generated!")
                                
                                with col_vid2:
                                    st.write("ğŸ“‹ **Video Settings:**")
                                    video_format = st.selectbox("Format", ["16:9 (YouTube)", "9:16 (TikTok)", "1:1 (Instagram)"])
                                    video_length = st.selectbox("Length", ["Full audio", "1 min clips", "30 sec clips"])
                                    add_subtitles = st.checkbox("Add subtitles", value=True)
                            else:
                                st.warning("ğŸ“· Please upload photos for your speakers to generate video")
                                st.info("ğŸ’¡ **Tip:** For best results:\nâ€¢ Use clear, front-facing photos\nâ€¢ Good lighting\nâ€¢ Neutral background")
                            
                            # Advanced features preview
                            with st.expander("ğŸš€ Advanced Features (Coming Soon)"):
                                st.write("**ğŸ¤– AI Enhancements:**")
                                st.write("â€¢ Real-time lip synchronization")
                                st.write("â€¢ Emotion-based expressions") 
                                st.write("â€¢ Auto gesture generation")
                                st.write("â€¢ Voice cloning integration")
                                st.write("â€¢ Multi-language dubbing")
                    
                    except Exception as e:
                        st.warning("Standard transcription failed, trying fallback method...")
                        # Use fallback audio loading
                        audio_data = load_audio_fallback(temp_path)
                        if audio_data is not None:
                            result = model.transcribe(audio_data)
                            st.text_area("Generated Transcript", result["text"], height=200)
                        else:
                            raise e
                    
                    # Cleanup
                    os.unlink(temp_path)
                    
                except Exception as e:
                    st.error(f"âŒ Error processing audio: {str(e)}")
                    st.info("ğŸ’¡ Try uploading a different audio format (WAV recommended)")
                    
                    # Cleanup on error
                    try:
                        os.unlink(temp_path)
                    except:
                        pass

    # About section
    st.header("ğŸ¤– About This AI Tool")
    st.write("""
    **Welcome to 2025's no-code AI revolution!** ğŸš€
    
    This tool demonstrates how anyone can build powerful AI applications without coding:
    
    ğŸ¯ **What it does:**
    â€¢ Converts audio to text using OpenAI's Whisper
    â€¢ Creates timestamped transcripts automatically
    â€¢ Prepares content for video generation
    
    ğŸ”® **Coming in 2025:**
    â€¢ AI avatar video generation
    â€¢ Multi-language support
    â€¢ Real-time processing
    â€¢ Social media integration
    """)
    
    st.divider()
    st.markdown("Made with â¤ï¸ using AI automation tools â€¢ 2025")

if __name__ == "__main__":
    main()