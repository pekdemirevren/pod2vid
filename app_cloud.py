import streamlit as st
import os
import tempfile
from pathlib import Path
import shutil
import subprocess

# Check FFmpeg availability
def check_ffmpeg():
    try:
        subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
        return True
    except:
        return False

# Lightweight imports
try:
    import whisper
    WHISPER_AVAILABLE = True
except:
    WHISPER_AVAILABLE = False
    st.error("Whisper not available in cloud environment")

# Audio loading fallback without FFmpeg
def load_audio_fallback(file_path):
    """Load audio without FFmpeg using librosa or soundfile"""
    try:
        import librosa
        audio, sr = librosa.load(file_path, sr=16000)
        return audio
    except:
        try:
            import soundfile as sf
            audio, sr = sf.read(file_path)
            # Resample if needed
            if sr != 16000:
                import librosa
                audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
            return audio
        except Exception as e:
            st.error(f"Audio loading failed: {e}")
            return None

def format_timestamp(seconds):
    """Convert seconds to MM:SS format"""
    minutes = int(seconds // 60)
    seconds = int(seconds % 60)
    return f"{minutes:02d}:{seconds:02d}"

def create_dialogue_preview(segments, speaker_photos):
    """Create a visual dialogue preview from transcript segments"""
    st.subheader("ğŸ­ Dialogue Preview")
    
    # Show how the video would look
    st.write("**This is how your dialogue video would appear:**")
    
    for i, segment in enumerate(segments[:5]):  # Show first 5 segments
        # Determine speaker
        speaker_num = (i % 2) + 1
        speaker_key = f"speaker{speaker_num}"
        
        # Create scene container
        with st.container():
            # Scene header
            scene_time = format_timestamp(segment["start"])
            scene_duration = segment["end"] - segment["start"]
            
            st.markdown(f"**ğŸ¬ Scene {i+1}** - *{scene_time}* ({scene_duration:.1f}s)")
            
            # Speaker and dialogue layout
            col_avatar, col_dialogue = st.columns([1, 3])
            
            with col_avatar:
                # Show speaker info
                st.markdown(f"**ğŸ‘¤ Speaker {speaker_num}**")
                
                if speaker_key in speaker_photos:
                    st.success("ğŸ“· Using uploaded photo")
                    st.caption("ğŸ­ Avatar animated")
                else:
                    st.info("ğŸ¤– AI generated avatar")
                    st.caption("ğŸ¨ Auto-created face")
                
                # Animation indicator
                st.markdown("ğŸ”„ *Speaking animation*")
            
            with col_dialogue:
                # Dialogue text with speech bubble effect
                dialogue_text = segment["text"].strip()
                
                st.markdown(f"""
                <div style="
                    background-color: #f0f2f6;
                    padding: 15px;
                    border-radius: 15px;
                    border-left: 4px solid #1f77b4;
                    margin: 10px 0;
                ">
                    <strong>ğŸ’¬ "{dialogue_text}"</strong>
                </div>
                """, unsafe_allow_html=True)
                
                # Speech effects
                word_count = len(dialogue_text.split())
                speech_duration = scene_duration
                st.caption(f"ğŸµ Lip-sync: {word_count} words in {speech_duration:.1f}s")
            
            st.divider()
    
    return True

def detect_speakers_from_text(text):
    """Detect dialogue patterns and estimate speaker count"""
    # Look for dialogue indicators
    dialogue_patterns = [
        "- ", "â€¢ ", "Speaker", "Host:", "Guest:", 
        "A:", "B:", "1:", "2:", "Q:", "A:"
    ]
    
    lines = text.split('\n')
    speaker_changes = 0
    
    for line in lines:
        for pattern in dialogue_patterns:
            if pattern in line:
                speaker_changes += 1
                break
    
    # Estimate speakers based on content length and patterns
    if speaker_changes > 5:
        return min(speaker_changes // 3, 4)  # Max 4 speakers
    elif "dialogue" in text.lower() or "conversation" in text.lower():
        return 2
    else:
        return 1

def display_timestamped_transcript(segments):
    """Display transcript with timestamps and speaker detection"""
    st.subheader("ğŸ“ Timestamped Transcript")
    
    # Analyze for dialogue patterns
    full_text = " ".join([seg["text"] for seg in segments])
    estimated_speakers = detect_speakers_from_text(full_text)
    
    if estimated_speakers > 1:
        st.info(f"ğŸ—£ï¸ Detected dialogue with approximately {estimated_speakers} speakers")
    
    # Create columns for better layout
    col1, col2 = st.columns([1, 4])
    
    current_speaker = 1
    for i, segment in enumerate(segments):
        # Simple speaker alternation for dialogue
        if estimated_speakers > 1 and i > 0:
            # Change speaker every few segments for dialogue
            if i % 3 == 0:  # Rough speaker change estimation
                current_speaker = 2 if current_speaker == 1 else 1
        
        with col1:
            start_time = format_timestamp(segment["start"])
            end_time = format_timestamp(segment["end"])
            if estimated_speakers > 1:
                st.write(f"**ğŸ‘¤ Speaker {current_speaker}**")
                st.write(f"*{start_time}-{end_time}*")
            else:
                st.write(f"**{start_time}-{end_time}**")
        
        with col2:
            text = segment["text"].strip()
            if estimated_speakers > 1:
                st.write(f"ğŸ’¬ {text}")
            else:
                st.write(text)
        
        st.divider()

def main():
    st.title("ğŸ™ï¸ AI Podcast Video Generator 2025")
    st.write("ğŸ¤– No-code AI automation: Audio â†’ Transcript â†’ Video")
    
    # Initialize session state
    if 'transcript_generated' not in st.session_state:
        st.session_state.transcript_generated = False
    if 'transcript_data' not in st.session_state:
        st.session_state.transcript_data = None
    if 'audio_file_name' not in st.session_state:
        st.session_state.audio_file_name = None
    if 'speaker_photos' not in st.session_state:
        st.session_state.speaker_photos = {}
    
    # Check system dependencies
    if not check_ffmpeg():
        st.warning("âš ï¸ FFmpeg not detected. Using fallback audio loading.")
    
    # Sidebar for options
    with st.sidebar:
        st.header("ğŸ›ï¸ Settings")
        model_size = st.selectbox("Whisper Model", ["tiny", "base", "small"], index=1)
        show_timestamps = st.checkbox("Show timestamps", value=True)
        generate_video = st.checkbox("Enable video generation", value=False)
        
        if generate_video:
            st.info("ğŸ¬ Video generation will be available after transcript")
        
        # Clear session button
        if st.button("ğŸ—‘ï¸ Clear All", help="Clear transcript and start over"):
            st.session_state.transcript_generated = False
            st.session_state.transcript_data = None
            st.session_state.audio_file_name = None
            st.session_state.speaker_photos = {}
            st.rerun()
    
    # File uploads
    st.header("ğŸ“ Upload Files")
    col1, col2 = st.columns(2)
    
    with col1:
        audio_file = st.file_uploader(
            "ğŸµ Upload audio file", 
            type=["wav", "mp3", "m4a", "ogg"],
            help="Upload podcast or audio file"
        )
    
    with col2:
        if generate_video:
            st.subheader("ğŸ‘¥ Speaker Photos")
            face_image1 = st.file_uploader(
                "ğŸ‘¤ Speaker 1 photo",
                type=["jpg", "png", "jpeg"],
                help="Photo of the first speaker",
                key="speaker1_upload"
            )
            if face_image1:
                st.session_state.speaker_photos['speaker1'] = face_image1
                st.success("âœ… Speaker 1 photo uploaded")
            
            face_image2 = st.file_uploader(
                "ğŸ‘¤ Speaker 2 photo", 
                type=["jpg", "png", "jpeg"],
                help="Photo of the second speaker",
                key="speaker2_upload"
            )
            if face_image2:
                st.session_state.speaker_photos['speaker2'] = face_image2
                st.success("âœ… Speaker 2 photo uploaded")
            
            # Optional: More speakers
            add_more_speakers = st.checkbox("â• More speakers?")
            if add_more_speakers:
                face_image3 = st.file_uploader(
                    "ğŸ‘¤ Speaker 3 photo",
                    type=["jpg", "png", "jpeg"],
                    key="speaker3_upload"
                )
                if face_image3:
                    st.session_state.speaker_photos['speaker3'] = face_image3
                    st.success("âœ… Speaker 3 photo uploaded")
    
    if audio_file and WHISPER_AVAILABLE:
        st.success(f"âœ… Uploaded: {audio_file.name}")
        
        # Store audio file name
        st.session_state.audio_file_name = audio_file.name
        
        if st.button("ğŸ¯ Generate Transcript", type="primary"):
            with st.spinner("ğŸ¤– AI processing your audio..."):
                try:
                    # Save uploaded file
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                        tmp_file.write(audio_file.read())
                        temp_path = tmp_file.name
                    
                    # Load model
                    st.info(f"Loading {model_size} model...")
                    model = whisper.load_model(model_size)
                    
                    # Try standard transcription first
                    try:
                        result = model.transcribe(temp_path, verbose=True)
                        
                        # Store transcript in session state
                        st.session_state.transcript_data = result
                        st.session_state.transcript_generated = True
                        
                        # Show success message
                        st.success("ğŸ‰ Transcript generated successfully!")
                        st.rerun()  # Refresh to show transcript
                        
                    except Exception as e:
                        st.warning("Standard transcription failed, trying fallback method...")
                        # Use fallback audio loading
                        audio_data = load_audio_fallback(temp_path)
                        if audio_data is not None:
                            result = model.transcribe(audio_data)
                            st.session_state.transcript_data = result
                            st.session_state.transcript_generated = True
                            st.success("ğŸ‰ Transcript generated with fallback method!")
                            st.rerun()
                        else:
                            raise e
                    
                    # Cleanup
                    os.unlink(temp_path)
                    
                except Exception as e:
                    st.error(f"âŒ Error processing audio: {str(e)}")
                    st.info("ğŸ’¡ Try uploading a different audio format (WAV recommended)")
                    
                    # Cleanup on error
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
    
    # Display transcript if generated
    if st.session_state.transcript_generated and st.session_state.transcript_data:
        result = st.session_state.transcript_data
        
        st.divider()
        st.header("ğŸ“‹ Generated Transcript")
        
        if show_timestamps:
            # Display timestamped transcript
            display_timestamped_transcript(result["segments"])
            
            # Download options
            st.subheader("ğŸ’¾ Download Options")
            col1, col2 = st.columns(2)
            
            with col1:
                # Simple transcript
                simple_transcript = result["text"]
                st.download_button(
                    "ğŸ“„ Download Simple Transcript",
                    simple_transcript,
                    file_name=f"transcript_{st.session_state.audio_file_name}.txt",
                    mime="text/plain"
                )
            
            with col2:
                # Timestamped transcript
                timestamped_text = ""
                for segment in result["segments"]:
                    start = format_timestamp(segment["start"])
                    end = format_timestamp(segment["end"])
                    text = segment["text"].strip()
                    timestamped_text += f"[{start}-{end}] {text}\n\n"
                
                st.download_button(
                    "â° Download Timestamped Transcript",
                    timestamped_text,
                    file_name=f"timestamped_{st.session_state.audio_file_name}.txt",
                    mime="text/plain"
                )
        else:
            # Simple transcript display
            st.subheader("ğŸ“ Transcript")
            st.text_area("Generated Transcript", result["text"], height=300)
        
        # Video generation section - now separate from transcript generation
        if generate_video:
            st.divider()
            st.header("ğŸ¬ Video Generation")
            
            # Check if any speaker photos uploaded
            speakers_uploaded = []
            if 'speaker1' in st.session_state.speaker_photos:
                speakers_uploaded.append("Speaker 1")
            if 'speaker2' in st.session_state.speaker_photos:
                speakers_uploaded.append("Speaker 2")
            if 'speaker3' in st.session_state.speaker_photos:
                speakers_uploaded.append("Speaker 3")
            
            if speakers_uploaded:
                st.success(f"âœ… Photos uploaded for: {', '.join(speakers_uploaded)}")
                
                # Speaker detection from transcript
                speaker_count = len([seg for seg in result["segments"] if "speaker" in seg.get("text", "").lower()])
                if speaker_count == 0:
                    # Estimate speakers from dialogue patterns
                    speaker_count = 2  # Default for dialogue
                
                st.info(f"ğŸ—£ï¸ Detected approximately {speaker_count} speakers in audio")
                
                col_vid1, col_vid2 = st.columns(2)
                
                with col_vid1:
                    if st.button("ğŸ¥ Generate Dialogue Video", key="generate_video_btn"):
                        st.balloons()
                        with st.spinner("ğŸ¬ Creating dialogue video..."):
                            # Simulate video generation process
                            progress = st.progress(0)
                            status = st.empty()
                            
                            steps = [
                                ("ğŸ” Analyzing dialogue structure...", 20),
                                ("ğŸ‘¥ Matching speakers to photos...", 40),
                                ("ğŸ­ Generating avatar animations...", 60), 
                                ("ğŸ¬ Rendering video scenes...", 80),
                                ("âœ¨ Adding transitions...", 100)
                            ]
                            
                            import time
                            for step_text, prog in steps:
                                status.text(step_text)
                                progress.progress(prog)
                                time.sleep(1)
                                
                            st.success("ğŸ‰ Dialogue video generated!")
                            
                            # Show realistic dialogue preview
                            create_dialogue_preview(result["segments"], st.session_state.speaker_photos)
                            
                            # Show video stats
                            total_duration = result["segments"][-1]["end"] if result["segments"] else 0
                            total_words = len(result["text"].split())
                            
                            col_stats1, col_stats2, col_stats3 = st.columns(3)
                            with col_stats1:
                                st.metric("Video Length", f"{format_timestamp(total_duration)}")
                            with col_stats2:
                                st.metric("Total Words", f"{total_words}")
                            with col_stats3:
                                st.metric("Speakers", f"{len(speakers_uploaded) or 2}")
                            
                            # Download section
                            st.subheader("ğŸ“± Download Your Video")
                            
                            col_dl1, col_dl2 = st.columns(2)
                            
                            with col_dl1:
                                # Create demo video content based on actual dialogue
                                video_content = f"""# Generated Video: {st.session_state.audio_file_name}
# Total Duration: {format_timestamp(total_duration) if 'total_duration' in locals() else 'Unknown'}
# Speakers: {len(speakers_uploaded) or 'Auto-detected'}

=== VIDEO SCRIPT ===
"""
                                if result and "segments" in result:
                                    for i, segment in enumerate(result["segments"]):
                                        speaker_num = (i % 2) + 1
                                        start_time = format_timestamp(segment["start"])
                                        end_time = format_timestamp(segment["end"])
                                        text = segment["text"].strip()
                                        video_content += f"\n[{start_time}-{end_time}] Speaker {speaker_num}: {text}\n"
                                
                                st.download_button(
                                    "ğŸ“„ Download Video Script",
                                    video_content.encode('utf-8'),
                                    file_name=f"video_script_{st.session_state.audio_file_name}.txt",
                                    mime="text/plain"
                                )
                            
                            with col_dl2:
                                # Simulated video file (in real implementation, this would be actual video)
                                st.download_button(
                                    "ğŸ¬ Download Video (Demo)",
                                    data=f"Demo video for: {st.session_state.audio_file_name}".encode('utf-8'),
                                    file_name=f"dialogue_{st.session_state.audio_file_name}.mp4",
                                    mime="video/mp4",
                                    help="This is a demo - real video would be generated here"
                                )
                            
                            # Real video preview message
                            st.info("""
                            ğŸ¬ **Real Video Preview:**
                            In production, this would show:
                            â€¢ Your uploaded speaker photos as animated avatars
                            â€¢ Lip-synchronized speech matching your audio
                            â€¢ Scene transitions between speakers
                            â€¢ Customized backgrounds and effects
                            """)
                            
                            # Technical details
                            with st.expander("ğŸ”§ Technical Details"):
                                st.write("**Video Generation Process:**")
                                st.write("1. ğŸ“Š Audio analysis and speaker detection")
                                st.write("2. ğŸ‘¤ Face photo processing and avatar creation") 
                                st.write("3. ğŸ­ Lip-sync animation generation")
                                st.write("4. ğŸ¬ Scene composition and rendering")
                                st.write("5. ğŸµ Audio-video synchronization")
                                st.write("6. ğŸ“± Format optimization for platforms")
                
                with col_vid2:
                    st.write("ğŸ“‹ **Video Settings:**")
                    video_format = st.selectbox("Format", ["16:9 (YouTube)", "9:16 (TikTok)", "1:1 (Instagram)"])
                    video_length = st.selectbox("Length", ["Full audio", "1 min clips", "30 sec clips"])
                    add_subtitles = st.checkbox("Add subtitles", value=True)
            else:
                st.warning("ğŸ“· Please upload photos for your speakers to generate video")
                st.info("ğŸ’¡ **Tip:** For best results:\nâ€¢ Use clear, front-facing photos\nâ€¢ Good lighting\nâ€¢ Neutral background")
            
            # Advanced features preview
            with st.expander("ğŸš€ Advanced Features (Coming Soon)"):
                st.write("**ğŸ¤– AI Enhancements:**")
                st.write("â€¢ Real-time lip synchronization")
                st.write("â€¢ Emotion-based expressions") 
                st.write("â€¢ Auto gesture generation")
                st.write("â€¢ Voice cloning integration")
                st.write("â€¢ Multi-language dubbing")

    # About section
    st.header("ğŸ¤– About This AI Tool")
    st.write("""
    **Welcome to 2025's no-code AI revolution!** ğŸš€
    
    This tool demonstrates how anyone can build powerful AI applications without coding:
    
    ğŸ¯ **What it does:**
    â€¢ Converts audio to text using OpenAI's Whisper
    â€¢ Creates timestamped transcripts automatically
    â€¢ Prepares content for video generation
    
    ğŸ”® **Coming in 2025:**
    â€¢ AI avatar video generation
    â€¢ Multi-language support
    â€¢ Real-time processing
    â€¢ Social media integration
    """)
    
    st.divider()
    st.markdown("Made with â¤ï¸ using AI automation tools â€¢ 2025")

if __name__ == "__main__":
    main()